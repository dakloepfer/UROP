{General}

# this name doesn't really matter, the position does
[training_options]
batch_size=4
max_epochs=130
max_test_iters=100
train_main_net_every_n_batches=5
# other schedulers possible but not yet implemented
learning_rate_scheduler=Linear
# Only needed if use Linear learning rate scheduler
decay_lr_over_last_n_epochs=20

[data_options]
regress_au_intensities=False

[loss_options]
reconstruction_loss_function=L1
au_regression_loss_function=BCEWithLogits
reconstruction_loss_lambda=1.0
# These don't matter for this one,
# I chose the ratios based on GANimation
gradient_loss_lambda=1.0
discriminator_loss_lambda=0.1

# I use for now as learning rates for main network and discriminator
# the generator/discriminator learning rates from GANimation
{Encoder}

[net]
learning_rate=0.0001
beta1=0.5
beta2=0.999
channels=3

# Architecture inspired by TCAE
[convolutional]
batch_normalise=1
filters=32
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=1
filters=64
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=1
filters=128
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=1
filters=256
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=1
filters=256
kernel_size=4
stride=2
pad=1
activation=relu

# AU Embedding would start here
[convolutional]
batch_normalise=1
filters=256
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=1
filters=256
kernel_size=4
stride=2
pad=1
activation=relu

[convolutional]
batch_normalise=0
filters=256
kernel_size=4
stride=2
pad=1
activation=None

{AUPredictor}

[net]
learning_rate=0.0001
beta1=0.5
beta2=0.999
# Set channels to 256 to avoid using discriminator
channels=256

# Same as TCAE used
[batch_norm]
num_features=256
eps=1e-5
batch_norm_dim=2

[connected]
# Number of action units
output_features=12
activation=linear
use_bias=False

{Decoder}

[net]
learning_rate=0.0001
beta1=0.5
beta2=0.999
# Set channels to 256 - same as output of encoder
channels=256
# Architecture inspired by TCAE

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=32
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=64
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=128
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=256
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=256
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=256
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=1
filters=256
kernel_size=3
stride=1
pad=1
activation=relu

[upsample]
output_sizes=None
scale_factor=2
mode=bilinear

[convolutional]
batch_normalise=0
filters=3
kernel_size=3
stride=1
pad=1
activation=tanh

# Doesn't matter here
{Discriminator}

[net]
learning_rate=0.0001
beta1=0.5
beta2=0.999
channels=1

# Just placeholder
[connected]
output_features=1
activation=linear
use_bias=False

# Do not add any further blocks in; they will not get
# used so will only add to the memory required