# Implements Attention: Encoder also returns Attention Block, which gets applied to
# z1 when z1 is fed into the AU Predictor
{General}

    # this name doesn't really matter, the position does
    [training_options]
        batch_size=32
        max_epochs=40
        max_test_iters=10
        train_main_net_every_n_batches=5
        # other schedulers possible but not yet implemented
        learning_rate_scheduler=Linear
        # Only needed if use Linear learning rate scheduler
        decay_lr_over_last_n_epochs=15

    [data_options]
        regress_au_intensities=False
        balance_dataset_with_momu=True
        epoch_size=97280
        sampling_size=32

    [loss_options]
        reconstruction_loss_function=L1
        au_regression_loss_function=BCEWithLogits
        reconstruction_loss_lambda=0.1
        # These don't matter for this one,
        # I chose the ratios based on GANimation
        gradient_loss_lambda=0.001
        discriminator_loss_lambda=0.01

# I use for now as learning rates for main network and discriminator
# the generator/discriminator learning rates from GANimation
{Encoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=3

    # Architecture inspired by TCAE
    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    # AU Embedding starts here
        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None


    # Attention Regression blocks for AU Prediction starts here,
    # branches off after one block of AU Embedding

        [route]
            branch_at_layer=4

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=sigmoid

    # Pose Embedding starts here
        [route]
            branch_at_layer=4

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=448
            kernel_size=4
            stride=2
            pad=1
            activation=None


    # Combine all attention, AU Embedding, and Pose Embedding
    [combine_concat]
        # Order is important now: Attention, AU Embedding, Pose Embedding
        combine_layers=11,7,-1
        dimension=1


{AUPredictor}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=64

    # Apply Attention
    # 1st AU
        [select_subset]
            origin_layer=-1
            dimension=1
            start_index=0
            end_index=64

        [select_subset]
            origin_layer=-2
            dimension=1
            start_index=64
            end_index=128

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            output_features=32
            activation=linear
            use_bias=False

        [batch_norm]
            num_features=32
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=12
            activation=linear
            use_bias=False

{Decoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        # Set channels to 1280 - same as output of encoder
        channels=576

    # Architecture inspired by TCAE

    # Remove Attention block
    [select_subset]
        origin_layer=-1
        dimension=1
        start_index=64
        end_index=576

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=0
        filters=3
        kernel_size=3
        stride=1
        pad=1
        activation=tanh

{Discriminator}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=576

    # Remove Attention block
    [select_subset]
        origin_layer=-1
        dimension=1
        start_index=64
        end_index=576

    # Architecture loosely inspired by Wasserstein AutoEncoder
    [connected]
        output_features=512
        activation=relu
        use_bias=True

    [connected]
        output_features=256
        activation=relu
        use_bias=True

    [connected]
        output_features=256
        activation=relu
        use_bias=True

    [connected]
        output_features=128
        activation=relu
        use_bias=True

    [connected]
        output_features=64
        activation=relu
        use_bias=True

    [connected]
        output_features=1
        activation=linear
        use_bias=True