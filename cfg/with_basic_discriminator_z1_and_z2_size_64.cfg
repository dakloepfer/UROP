# Roughly TCAE Encoder & Decoder, z1 and z2 sizes both 64, Discriminator with tapering, AUPredictor with one hidden layer
{General}

    # this name doesn't really matter, the position does
    [training_options]
        batch_size=32
        max_epochs=60
        max_test_iters=10
        train_main_net_every_n_batches=5
        # other schedulers possible but not yet implemented
        learning_rate_scheduler=Linear
        # Only needed if use Linear learning rate scheduler
        decay_lr_over_last_n_epochs=15

    [data_options]
        regress_au_intensities=False
        balance_dataset_with_momu=True
        epoch_size=97280
        sampling_size=32

    [loss_options]
        reconstruction_loss_function=L1
        au_regression_loss_function=BCEWithLogits
        reconstruction_loss_lambda=0.1
        # These don't matter for this one,
        # I chose the ratios based on GANimation
        gradient_loss_lambda=0.001
        discriminator_loss_lambda=0.01

# I use for now as learning rates for main network and discriminator
# the generator/discriminator learning rates from GANimation
{Encoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=3

    # Architecture inspired by TCAE
    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    # AU Embedding starts here
        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

    # Pose Embedding starts here
        [route]
            branch_at_layer=-4

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

    # Combine both embeddings
    [combine_concat]
        combine_layers=-1, -5
        dimension=1


{AUPredictor}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=64

    # Same as TCAE used
    [batch_norm]
        num_features=64
        eps=1e-5
        batch_norm_dim=2

    [connected]
        # Number of action units
        output_features=32
        activation=linear
        use_bias=False

    [batch_norm]
        num_features=32
        eps=1e-5
        batch_norm_dim=2

    [connected]
        # Number of action units
        output_features=12
        activation=linear
        use_bias=False

{Decoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        # Set channels to 128 - same as output of encoder
        channels=128
    # Architecture inspired by TCAE

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=0
        filters=3
        kernel_size=3
        stride=1
        pad=1
        activation=tanh

{Discriminator}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=128

    # Architecture loosely inspired by Wasserstein AutoEncoder
    [connected]
        output_features=128
        activation=relu
        use_bias=True

    [connected]
        output_features=64
        activation=relu
        use_bias=True

    [connected]
        output_features=32
        activation=relu
        use_bias=True

    [connected]
        output_features=1
        activation=leaky_relu
        use_bias=True