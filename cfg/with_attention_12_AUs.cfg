# Implements Attention: Encoder also returns Attention Block, which gets applied to
# z1 when z1 is fed into the AU Predictor
{General}

    # this name doesn't really matter, the position does
    [training_options]
        batch_size=32
        max_epochs=40
        max_test_iters=10
        train_main_net_every_n_batches=5
        # other schedulers possible but not yet implemented
        learning_rate_scheduler=Linear
        # Only needed if use Linear learning rate scheduler
        decay_lr_over_last_n_epochs=15

    [data_options]
        regress_au_intensities=False
        balance_dataset_with_momu=True
        epoch_size=97280
        sampling_size=32

    [loss_options]
        reconstruction_loss_function=L1
        au_regression_loss_function=BCEWithLogits
        reconstruction_loss_lambda=0.1
        # These don't matter for this one,
        # I chose the ratios based on GANimation
        gradient_loss_lambda=0.001
        discriminator_loss_lambda=0.01

# I use for now as learning rates for main network and discriminator
# the generator/discriminator learning rates from GANimation
{Encoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=3

    # Architecture inspired by TCAE
    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=4
        stride=2
        pad=1
        activation=relu

    # AU Embedding starts here
        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None


    # Attention Regression blocks for AU Prediction starts here,
    # branches off after one block of AU Embedding
    # One Attention block needed for every AU

        # 1st AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 2nd AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 3rd AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 4th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 5th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 6th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 7th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 8th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 9th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 10th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 11th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

        # 12th AU
        [route]
            branch_at_layer=5

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=64
            kernel_size=4
            stride=2
            pad=1
            activation=None

        [softmax]
            dim=1

    # Pose Embedding starts here
        [route]
            branch_at_layer=4

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=256
            kernel_size=4
            stride=2
            pad=1
            activation=relu

        [convolutional]
            batch_normalise=1
            filters=448
            kernel_size=4
            stride=2
            pad=1
            activation=None


    # Combine all attention, AU Embedding, and Pose Embedding
    [combine_concat]
        # Order is important now: Attention, AU Embedding, Pose Embedding
        combine_layers=11,15,19,23,27,31,35,39,43,47,51,54,7,-1
        dimension=1


{AUPredictor}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        # 12 x 64 + 64 for 12x Attention & z1
        channels=832

    # Identity Layer so I can easily reference input
    [identity]

    # Apply Attention
    # 1st AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=0
            end_index=64

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 2nd AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=64
            end_index=128

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 3rd AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=128
            end_index=192

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False
    
    # 4th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=192
            end_index=256

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False
    
    # 5th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=256
            end_index=320

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 6th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=320
            end_index=384

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 7th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=384
            end_index=448

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 8th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=448
            end_index=512

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 9th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=512
            end_index=576

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 10th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=576
            end_index=640

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 11th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=640
            end_index=704

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # 12th AU
        [select_subset]
            origin_layer=0
            dimension=1
            start_index=704
            end_index=768

        [select_subset]
            origin_layer=0
            dimension=1
            start_index=768
            end_index=832

        [combine_multiply]
            combine_layers=-1, -2

        # Same as TCAE used
        [batch_norm]
            num_features=64
            eps=1e-5
            batch_norm_dim=2

        [connected]
            # Number of action units
            output_features=1
            activation=linear
            use_bias=False

    # combine all 12 AU predictions
    [combine_concat]
        combine_layers=5,10,15,20,25,30,35,40,45,50,55,60
        dimension=1

{Decoder}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        # Set channels to 1280 - same as output of encoder
        channels=1280

    # Architecture inspired by TCAE

    # Remove Attention block
    [select_subset]
        origin_layer=-1
        dimension=1
        start_index=768
        end_index=1280

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=32
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=64
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=128
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=1
        filters=256
        kernel_size=3
        stride=1
        pad=1
        activation=relu

    [upsample]
        output_sizes=None
        scale_factor=2
        mode=bilinear

    [convolutional]
        batch_normalise=0
        filters=3
        kernel_size=3
        stride=1
        pad=1
        activation=tanh

{Discriminator}

    [net]
        learning_rate=0.0001
        beta1=0.5
        beta2=0.999
        channels=1280

    # Remove Attention block
    [select_subset]
        origin_layer=-1
        dimension=1
        start_index=768
        end_index=1280

    # Architecture loosely inspired by Wasserstein AutoEncoder
    [connected]
        output_features=512
        activation=relu
        use_bias=True

    [connected]
        output_features=256
        activation=relu
        use_bias=True

    [connected]
        output_features=256
        activation=relu
        use_bias=True

    [connected]
        output_features=256
        activation=relu
        use_bias=True

    [connected]
        output_features=1
        activation=leaky_relu
        use_bias=True